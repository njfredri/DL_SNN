{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.0.1+cu118\n",
      "GPU availability:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print('Pytorch version: ', torch.__version__)\n",
    "print('GPU availability: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_and_fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #including bias, evern though the SNN did not\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5*5*12, 10)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.conv_and_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().cuda()\n",
    "EPOCHS=50 #set to 50 epochs bc of diminishing returns\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/CNN\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model saved\n",
      "==>>> epoch: 0, train loss: 0.000698\n",
      "new best model saved\n",
      "==>>> epoch: 1, train loss: 0.000484\n",
      "new best model saved\n",
      "==>>> epoch: 2, train loss: 0.000443\n",
      "new best model saved\n",
      "==>>> epoch: 3, train loss: 0.000419\n",
      "new best model saved\n",
      "==>>> epoch: 4, train loss: 0.000402\n",
      "new best model saved\n",
      "==>>> epoch: 5, train loss: 0.000391\n",
      "new best model saved\n",
      "==>>> epoch: 6, train loss: 0.000382\n",
      "new best model saved\n",
      "==>>> epoch: 7, train loss: 0.000375\n",
      "new best model saved\n",
      "==>>> epoch: 8, train loss: 0.000370\n",
      "new best model saved\n",
      "==>>> epoch: 9, train loss: 0.000365\n",
      "new best model saved\n",
      "==>>> epoch: 10, train loss: 0.000364\n",
      "new best model saved\n",
      "==>>> epoch: 11, train loss: 0.000359\n",
      "new best model saved\n",
      "==>>> epoch: 12, train loss: 0.000357\n",
      "new best model saved\n",
      "==>>> epoch: 13, train loss: 0.000356\n",
      "new best model saved\n",
      "==>>> epoch: 14, train loss: 0.000353\n",
      "new best model saved\n",
      "==>>> epoch: 15, train loss: 0.000351\n",
      "new best model saved\n",
      "==>>> epoch: 16, train loss: 0.000349\n",
      "new best model saved\n",
      "==>>> epoch: 17, train loss: 0.000347\n",
      "new best model saved\n",
      "==>>> epoch: 18, train loss: 0.000346\n",
      "new best model saved\n",
      "==>>> epoch: 19, train loss: 0.000346\n",
      "new best model saved\n",
      "==>>> epoch: 20, train loss: 0.000344\n",
      "new best model saved\n",
      "==>>> epoch: 21, train loss: 0.000343\n",
      "new best model saved\n",
      "==>>> epoch: 22, train loss: 0.000343\n",
      "new best model saved\n",
      "==>>> epoch: 23, train loss: 0.000342\n",
      "new best model saved\n",
      "==>>> epoch: 24, train loss: 0.000341\n",
      "new best model saved\n",
      "==>>> epoch: 25, train loss: 0.000340\n",
      "new best model saved\n",
      "==>>> epoch: 26, train loss: 0.000339\n",
      "==>>> epoch: 27, train loss: 0.000340\n",
      "new best model saved\n",
      "==>>> epoch: 28, train loss: 0.000338\n",
      "new best model saved\n",
      "==>>> epoch: 29, train loss: 0.000337\n",
      "new best model saved\n",
      "==>>> epoch: 30, train loss: 0.000337\n",
      "new best model saved\n",
      "==>>> epoch: 31, train loss: 0.000337\n",
      "new best model saved\n",
      "==>>> epoch: 32, train loss: 0.000336\n",
      "new best model saved\n",
      "==>>> epoch: 33, train loss: 0.000335\n",
      "==>>> epoch: 34, train loss: 0.000336\n",
      "new best model saved\n",
      "==>>> epoch: 35, train loss: 0.000335\n",
      "new best model saved\n",
      "==>>> epoch: 36, train loss: 0.000334\n",
      "new best model saved\n",
      "==>>> epoch: 37, train loss: 0.000334\n",
      "new best model saved\n",
      "==>>> epoch: 38, train loss: 0.000334\n",
      "new best model saved\n",
      "==>>> epoch: 39, train loss: 0.000333\n",
      "==>>> epoch: 40, train loss: 0.000333\n",
      "new best model saved\n",
      "==>>> epoch: 41, train loss: 0.000333\n",
      "new best model saved\n",
      "==>>> epoch: 42, train loss: 0.000332\n",
      "new best model saved\n",
      "==>>> epoch: 43, train loss: 0.000332\n",
      "new best model saved\n",
      "==>>> epoch: 44, train loss: 0.000331\n",
      "new best model saved\n",
      "==>>> epoch: 45, train loss: 0.000331\n",
      "new best model saved\n",
      "==>>> epoch: 46, train loss: 0.000331\n",
      "new best model saved\n",
      "==>>> epoch: 47, train loss: 0.000330\n",
      "new best model saved\n",
      "==>>> epoch: 48, train loss: 0.000330\n",
      "new best model saved\n",
      "==>>> epoch: 49, train loss: 0.000330\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "full_path = './Models/CNN.pt'\n",
    "best_loss = 1000000.0\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        out = model(x)\n",
    "        target_onehot = F.one_hot(target, 10).float()\n",
    "        loss = F.mse_loss(out, target_onehot) #this is done in order to match the SNN loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / len(train_set)\n",
    "    if avg_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f=full_path)\n",
    "        best_loss = avg_loss\n",
    "        print('new best model saved')\n",
    "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.5720946788787842\n",
      "test loss: 0.000369, test accuracy: 0.874500\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "full_path = './Models/CNN.pt'\n",
    "checkpoint = torch.load(f=full_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "total_loss = 0\n",
    "correct_cnt = 0\n",
    "model.eval()\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.cuda(), target.cuda()\n",
    "    out = model(x)\n",
    "    target_onehot = F.one_hot(target, 10).float()\n",
    "    loss = F.mse_loss(out, target_onehot)\n",
    "\n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_cnt += (pred_label == target).sum()\n",
    "    # smooth average\n",
    "    total_loss += loss.item()\n",
    "\n",
    "#time just inference\n",
    "start = time.time()\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.cuda(), target.cuda()\n",
    "    out = model(x)\n",
    "end = time.time()\n",
    "\n",
    "avg_loss = total_loss / len(test_set)\n",
    "avg_acc = correct_cnt / len(test_set)\n",
    "print('Time: ' + str(end-start))\n",
    "print(f'test loss: {avg_loss:.6f}, test accuracy: {avg_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\DL_SNN\\DL_SNN\\CNN_Benchmark.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (x, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     x, target \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcuda(), target\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     out \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\DL_SNN\\DL_SNN\\CNN_Benchmark.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_and_fc(x)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#infinite loop to see power difference\n",
    "while True:\n",
    "    start = time.time()\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        out = model(x)\n",
    "    end = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
