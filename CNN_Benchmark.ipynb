{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version:  2.0.1+cu118\n",
      "GPU availability:  True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print('Pytorch version: ', torch.__version__)\n",
    "print('GPU availability: ', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_and_fc = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #including bias, evern though the SNN did not\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5),\n",
    "            nn.BatchNorm2d(12),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(5*5*12, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "            \n",
    "            )\n",
    "    def forward(self, x):\n",
    "        return self.conv_and_fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().cuda()\n",
    "EPOCHS=50 #set to 50 epochs bc of diminishing returns\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/CNN\"\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model saved\n",
      "==>>> epoch: 0, train loss: 0.000406\n",
      "new best model saved\n",
      "==>>> epoch: 1, train loss: 0.000282\n",
      "new best model saved\n",
      "==>>> epoch: 2, train loss: 0.000252\n",
      "new best model saved\n",
      "==>>> epoch: 3, train loss: 0.000235\n",
      "new best model saved\n",
      "==>>> epoch: 4, train loss: 0.000220\n",
      "new best model saved\n",
      "==>>> epoch: 5, train loss: 0.000210\n",
      "new best model saved\n",
      "==>>> epoch: 6, train loss: 0.000200\n",
      "new best model saved\n",
      "==>>> epoch: 7, train loss: 0.000192\n",
      "new best model saved\n",
      "==>>> epoch: 8, train loss: 0.000184\n",
      "new best model saved\n",
      "==>>> epoch: 9, train loss: 0.000178\n",
      "new best model saved\n",
      "==>>> epoch: 10, train loss: 0.000171\n",
      "new best model saved\n",
      "==>>> epoch: 11, train loss: 0.000164\n",
      "new best model saved\n",
      "==>>> epoch: 12, train loss: 0.000159\n",
      "new best model saved\n",
      "==>>> epoch: 13, train loss: 0.000153\n",
      "new best model saved\n",
      "==>>> epoch: 14, train loss: 0.000148\n",
      "new best model saved\n",
      "==>>> epoch: 15, train loss: 0.000143\n",
      "new best model saved\n",
      "==>>> epoch: 16, train loss: 0.000139\n",
      "new best model saved\n",
      "==>>> epoch: 17, train loss: 0.000133\n",
      "new best model saved\n",
      "==>>> epoch: 18, train loss: 0.000130\n",
      "new best model saved\n",
      "==>>> epoch: 19, train loss: 0.000124\n",
      "new best model saved\n",
      "==>>> epoch: 20, train loss: 0.000120\n",
      "new best model saved\n",
      "==>>> epoch: 21, train loss: 0.000119\n",
      "new best model saved\n",
      "==>>> epoch: 22, train loss: 0.000113\n",
      "new best model saved\n",
      "==>>> epoch: 23, train loss: 0.000111\n",
      "new best model saved\n",
      "==>>> epoch: 24, train loss: 0.000108\n",
      "new best model saved\n",
      "==>>> epoch: 25, train loss: 0.000105\n",
      "new best model saved\n",
      "==>>> epoch: 26, train loss: 0.000102\n",
      "new best model saved\n",
      "==>>> epoch: 27, train loss: 0.000100\n",
      "new best model saved\n",
      "==>>> epoch: 28, train loss: 0.000097\n",
      "new best model saved\n",
      "==>>> epoch: 29, train loss: 0.000096\n",
      "new best model saved\n",
      "==>>> epoch: 30, train loss: 0.000092\n",
      "new best model saved\n",
      "==>>> epoch: 31, train loss: 0.000090\n",
      "new best model saved\n",
      "==>>> epoch: 32, train loss: 0.000088\n",
      "new best model saved\n",
      "==>>> epoch: 33, train loss: 0.000085\n",
      "==>>> epoch: 34, train loss: 0.000086\n",
      "new best model saved\n",
      "==>>> epoch: 35, train loss: 0.000081\n",
      "new best model saved\n",
      "==>>> epoch: 36, train loss: 0.000079\n",
      "==>>> epoch: 37, train loss: 0.000079\n",
      "new best model saved\n",
      "==>>> epoch: 38, train loss: 0.000077\n",
      "new best model saved\n",
      "==>>> epoch: 39, train loss: 0.000076\n",
      "new best model saved\n",
      "==>>> epoch: 40, train loss: 0.000072\n",
      "==>>> epoch: 41, train loss: 0.000073\n",
      "new best model saved\n",
      "==>>> epoch: 42, train loss: 0.000071\n",
      "new best model saved\n",
      "==>>> epoch: 43, train loss: 0.000070\n",
      "new best model saved\n",
      "==>>> epoch: 44, train loss: 0.000069\n",
      "new best model saved\n",
      "==>>> epoch: 45, train loss: 0.000068\n",
      "new best model saved\n",
      "==>>> epoch: 46, train loss: 0.000066\n",
      "new best model saved\n",
      "==>>> epoch: 47, train loss: 0.000065\n",
      "new best model saved\n",
      "==>>> epoch: 48, train loss: 0.000064\n",
      "new best model saved\n",
      "==>>> epoch: 49, train loss: 0.000062\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "full_path = './Models/CNN.pt'\n",
    "best_loss = 1000000.0\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        out = model(x)\n",
    "        target_onehot = F.one_hot(target, 10).float()\n",
    "        loss = F.mse_loss(out, target_onehot) #this is done in order to match the SNN loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / len(train_set)\n",
    "    if avg_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f=full_path)\n",
    "        best_loss = avg_loss\n",
    "        print('new best model saved')\n",
    "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.5599989891052246\n",
      "test loss: 0.000261, test accuracy: 0.897500\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "full_path = './Models/CNN.pt'\n",
    "checkpoint = torch.load(f=full_path)\n",
    "model.load_state_dict(checkpoint)\n",
    "total_loss = 0\n",
    "correct_cnt = 0\n",
    "model.eval()\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.cuda(), target.cuda()\n",
    "    out = model(x)\n",
    "    target_onehot = F.one_hot(target, 10).float()\n",
    "    loss = F.mse_loss(out, target_onehot)\n",
    "\n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_cnt += (pred_label == target).sum()\n",
    "    # smooth average\n",
    "    total_loss += loss.item()\n",
    "\n",
    "#time just inference\n",
    "start = time.time()\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.cuda(), target.cuda()\n",
    "    out = model(x)\n",
    "end = time.time()\n",
    "\n",
    "avg_loss = total_loss / len(test_set)\n",
    "avg_acc = correct_cnt / len(test_set)\n",
    "print('Time: ' + str(end-start))\n",
    "print(f'test loss: {avg_loss:.6f}, test accuracy: {avg_acc:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nathan\\Desktop\\DL_SNN\\DL_SNN\\CNN_Benchmark.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch_idx, (x, target) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(test_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         x, target \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mcuda(), target\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nathan/Desktop/DL_SNN/DL_SNN/CNN_Benchmark.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         out \u001b[39m=\u001b[39m model(x)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\Nathan\\.conda\\envs\\python_env1\\lib\\site-packages\\torchvision\\transforms\\functional.py:170\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m pic\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    169\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39m255\u001b[39m \u001b[39m*\u001b[39m img\n\u001b[1;32m--> 170\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mview(pic\u001b[39m.\u001b[39;49msize[\u001b[39m1\u001b[39;49m], pic\u001b[39m.\u001b[39;49msize[\u001b[39m0\u001b[39;49m], F_pil\u001b[39m.\u001b[39;49mget_image_num_channels(pic))\n\u001b[0;32m    171\u001b[0m \u001b[39m# put it from HWC to CHW format\u001b[39;00m\n\u001b[0;32m    172\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#infinite loop to see power difference\n",
    "while True:\n",
    "    start = time.time()\n",
    "    for batch_idx, (x, target) in enumerate(test_loader):\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        out = model(x)\n",
    "    end = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
