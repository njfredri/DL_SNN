{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# spikingjelly.activation_based.examples.conv_fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from spikingjelly.activation_based import encoding, monitor\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from torch.cuda import amp\n",
    "import sys\n",
    "import datetime\n",
    "from spikingjelly import visualizing\n",
    "import numpy as np\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN(nn.Module):\n",
    "    def __init__(self, T: int):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.conv_and_fc = nn.Sequential(\n",
    "            layer.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2, bias=False),\n",
    "            layer.BatchNorm2d(6),\n",
    "            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Conv2d(in_channels=6, out_channels=12, kernel_size=5, bias=False),\n",
    "            layer.BatchNorm2d(12),\n",
    "            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Flatten(),\n",
    "            layer.Linear(5*5*12, 10, bias=False),\n",
    "            neuron.LIFNode(surrogate_function=surrogate.ATan()),\n",
    "            # layer.Linear(120, 84),\n",
    "            # neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            # layer.Linear(84, 10),\n",
    "            # neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "            )\n",
    "        functional.set_step_mode(self, step_mode='m')\n",
    "    def forward(self, x):\n",
    "        x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n",
    "        z = self.conv_and_fc(x_seq)\n",
    "        fr = z.mean(0)\n",
    "        return fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau = 2.0\n",
    "timesteps = 40\n",
    "model = SCNN(T=timesteps).to(device=device)\n",
    "EPOCHS=100\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/CSNN\"\n",
    "encoder = encoding.PoissonEncoder()\n",
    "batch_size=64\n",
    "num_workers = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=train_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=test_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if AMP:\n",
    "    scaler = amp.GradScaler()\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "writer = SummaryWriter(out_dir, purge_step=0)\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write('\\n')\n",
    "    args_txt.write(' '.join(sys.argv))\n",
    "file_dir = './Models/'\n",
    "if not os.path.exists(file_dir):\n",
    "    os.makedirs(file_dir)\n",
    "    print(f'Mkdir {file_dir}.')\n",
    "full_path = file_dir + '/CSNN.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model saved\n",
      "epoch: 0; loss1880.071486890316\n",
      "new best model saved\n",
      "epoch: 1; loss1328.5742701888084\n",
      "new best model saved\n",
      "epoch: 2; loss1220.2118311524391\n",
      "new best model saved\n",
      "epoch: 3; loss1158.671469926834\n",
      "new best model saved\n",
      "epoch: 4; loss1118.4579518437386\n",
      "new best model saved\n",
      "epoch: 5; loss1092.347270399332\n",
      "new best model saved\n",
      "epoch: 6; loss1067.9730680286884\n",
      "new best model saved\n",
      "epoch: 7; loss1045.5461457967758\n",
      "new best model saved\n",
      "epoch: 8; loss1033.0673846006393\n",
      "new best model saved\n",
      "epoch: 9; loss1016.8942696750164\n",
      "new best model saved\n",
      "epoch: 10; loss1005.9954246878624\n",
      "new best model saved\n",
      "epoch: 11; loss993.9650643467903\n",
      "new best model saved\n",
      "epoch: 12; loss981.9952650368214\n",
      "new best model saved\n",
      "epoch: 13; loss975.672704577446\n",
      "new best model saved\n",
      "epoch: 14; loss970.6636627316475\n",
      "new best model saved\n",
      "epoch: 15; loss958.3928230702877\n",
      "new best model saved\n",
      "epoch: 16; loss953.7237045019865\n",
      "new best model saved\n",
      "epoch: 17; loss947.1482636332512\n",
      "new best model saved\n",
      "epoch: 18; loss942.61514544487\n",
      "new best model saved\n",
      "epoch: 19; loss939.1353027224541\n",
      "new best model saved\n",
      "epoch: 20; loss933.035300642252\n",
      "new best model saved\n",
      "epoch: 21; loss931.1275794506073\n",
      "epoch: 22; loss931.611466139555\n",
      "new best model saved\n",
      "epoch: 23; loss923.1188206076622\n",
      "new best model saved\n",
      "epoch: 24; loss918.7388192564249\n",
      "new best model saved\n",
      "epoch: 25; loss918.540542781353\n",
      "new best model saved\n",
      "epoch: 26; loss915.0170625150204\n",
      "new best model saved\n",
      "epoch: 27; loss912.5287001281977\n",
      "epoch: 28; loss913.055740147829\n",
      "new best model saved\n",
      "epoch: 29; loss904.8550641238689\n",
      "epoch: 30; loss906.5746618360281\n",
      "new best model saved\n",
      "epoch: 31; loss899.6769448667765\n",
      "epoch: 32; loss900.8245799839497\n",
      "epoch: 33; loss900.3324988484383\n",
      "new best model saved\n",
      "epoch: 34; loss894.5679820179939\n",
      "epoch: 35; loss895.1823436617851\n",
      "new best model saved\n",
      "epoch: 36; loss890.7438212037086\n",
      "new best model saved\n",
      "epoch: 37; loss889.4569787085056\n",
      "epoch: 38; loss891.4381018280983\n",
      "epoch: 39; loss891.7597409784794\n",
      "epoch: 40; loss890.2209794521332\n",
      "new best model saved\n",
      "epoch: 41; loss886.2225800454617\n",
      "new best model saved\n",
      "epoch: 42; loss883.2631402015686\n",
      "new best model saved\n",
      "epoch: 43; loss882.1763796508312\n",
      "epoch: 44; loss882.6084197759628\n",
      "new best model saved\n",
      "epoch: 45; loss879.5512597858906\n",
      "new best model saved\n",
      "epoch: 46; loss875.2801792025566\n",
      "epoch: 47; loss878.2718588858843\n",
      "epoch: 48; loss876.1010221540928\n",
      "new best model saved\n",
      "epoch: 49; loss870.7146213948727\n",
      "new best model saved\n",
      "epoch: 50; loss869.9847809523344\n",
      "new best model saved\n",
      "epoch: 51; loss869.9104607403278\n",
      "epoch: 52; loss870.0031402111053\n",
      "new best model saved\n",
      "epoch: 53; loss869.1545398533344\n",
      "epoch: 54; loss870.2457821369171\n",
      "new best model saved\n",
      "epoch: 55; loss865.2997785657644\n",
      "new best model saved\n",
      "epoch: 56; loss862.940619379282\n",
      "new best model saved\n",
      "epoch: 57; loss861.5484599173069\n",
      "new best model saved\n",
      "epoch: 58; loss859.4946600496769\n",
      "epoch: 59; loss859.6634204536676\n",
      "epoch: 60; loss861.4184601008892\n",
      "new best model saved\n",
      "epoch: 61; loss858.1792995035648\n",
      "new best model saved\n",
      "epoch: 62; loss858.1466973423958\n",
      "new best model saved\n",
      "epoch: 63; loss857.3986170291901\n",
      "new best model saved\n",
      "epoch: 64; loss853.9317406713963\n",
      "epoch: 65; loss855.9478197693825\n",
      "new best model saved\n",
      "epoch: 66; loss848.617938131094\n",
      "epoch: 67; loss853.0764619112015\n",
      "epoch: 68; loss855.1310168206692\n",
      "new best model saved\n",
      "epoch: 69; loss848.2568188905716\n",
      "epoch: 70; loss850.3675390779972\n",
      "epoch: 71; loss850.2851377427578\n",
      "new best model saved\n",
      "epoch: 72; loss845.9612607657909\n",
      "epoch: 73; loss849.8912597894669\n",
      "epoch: 74; loss846.3128999322653\n",
      "new best model saved\n",
      "epoch: 75; loss844.4473405480385\n",
      "new best model saved\n",
      "epoch: 76; loss843.2471009492874\n",
      "epoch: 77; loss849.623182028532\n",
      "epoch: 78; loss845.6833395063877\n",
      "new best model saved\n",
      "epoch: 79; loss842.1530588716269\n",
      "new best model saved\n",
      "epoch: 80; loss841.4449003636837\n",
      "epoch: 81; loss842.7406585663557\n",
      "epoch: 82; loss842.6789779663086\n",
      "new best model saved\n",
      "epoch: 83; loss840.2456988990307\n",
      "epoch: 84; loss842.0499390214682\n",
      "epoch: 85; loss841.6425002217293\n",
      "new best model saved\n",
      "epoch: 86; loss837.802782163024\n",
      "epoch: 87; loss838.747659444809\n",
      "new best model saved\n",
      "epoch: 88; loss837.4972994327545\n",
      "new best model saved\n",
      "epoch: 89; loss835.8908184766769\n",
      "new best model saved\n",
      "epoch: 90; loss833.8527808189392\n",
      "epoch: 91; loss836.9865375310183\n",
      "epoch: 92; loss834.8678581416607\n",
      "epoch: 93; loss837.1546207666397\n",
      "epoch: 94; loss837.5867393612862\n",
      "epoch: 95; loss836.627979144454\n",
      "new best model saved\n",
      "epoch: 96; loss832.8336196094751\n",
      "epoch: 97; loss834.8985407948494\n",
      "new best model saved\n",
      "epoch: 98; loss828.6585783809423\n",
      "epoch: 99; loss830.7782589942217\n"
     ]
    }
   ],
   "source": [
    "functional.reset_net(model)\n",
    "check = input('running this will remove the old saved model')\n",
    "best_loss = 10000000.0\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for x, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "\n",
    "        if scaler is None:\n",
    "            out_fr = 0.\n",
    "            # for t in range(timesteps):\n",
    "            #     encoded_img = encoder(x)\n",
    "            #     out_fr += model(encoded_img)\n",
    "            # out_fr = out_fr / timesteps\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with amp.autocast():\n",
    "\n",
    "                out_fr = model(x)\n",
    "                # out_fr = out_fr / timesteps\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        # print(out_fr.shape)\n",
    "        # print(label.shape)\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(model) #need to reset the snn before reuse\n",
    "    if train_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f=full_path)\n",
    "        best_loss = train_loss\n",
    "        print('new best model saved')\n",
    "    print('epoch: ' + str(epoch) + '; loss' + str(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_dir = './Models/'\n",
    "full_path = file_dir + '/CSNN.pt'\n",
    "checkpoint = torch.load(f=full_path)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6539; loss: 576.0729441046715\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "for x, label in test_loader:\n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(device)\n",
    "    label = label.to(device)\n",
    "    label_onehot = F.one_hot(label, 10).float()\n",
    "    out_fr = model(x)\n",
    "    loss = F.mse_loss(out_fr, label_onehot)\n",
    "    \n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "    functional.reset_net(model) #need to reset the snn before reuse\n",
    "test_acc = test_acc/test_samples\n",
    "print('acc: ' + str(test_acc) + '; loss: ' + str(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of Spikes for power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6539; loss: 576.0729424357414\n",
      "SCNN(\n",
      "  (conv_and_fc): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), step_mode=m)\n",
      "    (1): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): LeakyKReLU()\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, step_mode=m)\n",
      "    (3): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1), step_mode=m)\n",
      "    (4): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): LeakyKReLU()\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False, step_mode=m)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1, step_mode=m)\n",
      "    (7): Linear(in_features=300, out_features=10, bias=True)\n",
      "    (8): IFNode(\n",
      "      v_threshold=1.0, v_reset=0.0, detach_reset=False, step_mode=m, backend=torch\n",
      "      (surrogate_function): LeakyKReLU()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "total spikes: 0\n",
      "spike_seq_monitor.records=\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "spike_monitor = monitor.OutputMonitor(model, neuron.LIFNode)\n",
    "start_time = time.time()\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "for x, label in test_loader:\n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(device)\n",
    "    label = label.to(device)\n",
    "    label_onehot = F.one_hot(label, 10).float()\n",
    "    out_fr = model(x)\n",
    "    loss = F.mse_loss(out_fr, label_onehot)\n",
    "    \n",
    "    test_samples += label.numel()\n",
    "    test_loss += loss.item() * label.numel()\n",
    "    test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "    functional.reset_net(model) #need to reset the snn before reuse\n",
    "test_acc = test_acc/test_samples\n",
    "print('acc: ' + str(test_acc) + '; loss: ' + str(test_loss))\n",
    "print(model)\n",
    "total_spikes = 0\n",
    "for tens in spike_monitor.records:\n",
    "    tensnp = tens.cpu().numpy()\n",
    "    total_spikes += np.sum(tensnp)\n",
    "print('total spikes: ' + str(total_spikes))\n",
    "print(f'spike_seq_monitor.records=\\n{len(spike_monitor.records)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
