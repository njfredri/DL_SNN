{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# spikingjelly.activation_based.examples.conv_fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from spikingjelly.activation_based import encoding\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from torch.cuda import amp\n",
    "import sys\n",
    "import datetime\n",
    "from spikingjelly import visualizing\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCNN(nn.Module):\n",
    "    def __init__(self, T: int):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.conv_and_fc = nn.Sequential(\n",
    "            layer.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Conv2d(in_channels=6, out_channels=12, kernel_size=5),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Flatten(),\n",
    "            layer.Linear(5*5*12, 10),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            # layer.Linear(120, 84),\n",
    "            # neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            # layer.Linear(84, 10),\n",
    "            # neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "            )\n",
    "        functional.set_step_mode(self, step_mode='m')\n",
    "    def forward(self, x):\n",
    "        x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n",
    "        z = self.conv_and_fc(x_seq)\n",
    "        fr = z.mean(0)\n",
    "        return fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau = 2.0\n",
    "timesteps = 40\n",
    "model = SCNN(T=timesteps).to(device=device)\n",
    "EPOCHS=100\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.00001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/CSNN\"\n",
    "encoder = encoding.PoissonEncoder()\n",
    "batch_size=64\n",
    "num_workers = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=train_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=test_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if AMP:\n",
    "    scaler = amp.GradScaler()\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "writer = SummaryWriter(out_dir, purge_step=0)\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write('\\n')\n",
    "    args_txt.write(' '.join(sys.argv))\n",
    "file_dir = './Models/'\n",
    "if not os.path.exists(file_dir):\n",
    "    os.makedirs(file_dir)\n",
    "    print(f'Mkdir {file_dir}.')\n",
    "full_path = file_dir + '/CSNN.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best model saved\n",
      "epoch: 0; loss5556.556187152863\n",
      "new best model saved\n",
      "epoch: 1; loss4791.5078320503235\n",
      "new best model saved\n",
      "epoch: 2; loss4490.977624416351\n",
      "new best model saved\n",
      "epoch: 3; loss4340.243544578552\n",
      "new best model saved\n",
      "epoch: 4; loss4246.435659408569\n",
      "new best model saved\n",
      "epoch: 5; loss4105.152578115463\n",
      "new best model saved\n",
      "epoch: 6; loss3975.1210095882416\n",
      "new best model saved\n",
      "epoch: 7; loss3905.2128167152405\n",
      "new best model saved\n",
      "epoch: 8; loss3851.2688777446747\n",
      "new best model saved\n",
      "epoch: 9; loss3805.5682961940765\n",
      "new best model saved\n",
      "epoch: 10; loss3763.6618542671204\n",
      "new best model saved\n",
      "epoch: 11; loss3738.7917115688324\n",
      "new best model saved\n",
      "epoch: 12; loss3723.7766494750977\n",
      "new best model saved\n",
      "epoch: 13; loss3706.3287122249603\n",
      "new best model saved\n",
      "epoch: 14; loss3689.5630280971527\n",
      "new best model saved\n",
      "epoch: 15; loss3673.541717529297\n",
      "new best model saved\n",
      "epoch: 16; loss3658.6415803432465\n",
      "new best model saved\n",
      "epoch: 17; loss3646.0450201034546\n",
      "new best model saved\n",
      "epoch: 18; loss3637.4543437957764\n",
      "new best model saved\n",
      "epoch: 19; loss3626.1111450195312\n",
      "new best model saved\n",
      "epoch: 20; loss3616.3210818767548\n",
      "new best model saved\n",
      "epoch: 21; loss3601.184147834778\n",
      "new best model saved\n",
      "epoch: 22; loss3593.979212999344\n",
      "new best model saved\n",
      "epoch: 23; loss3586.0920889377594\n",
      "epoch: 24; loss3586.5317752361298\n",
      "new best model saved\n",
      "epoch: 25; loss3579.24627161026\n",
      "epoch: 26; loss3585.7608954906464\n",
      "epoch: 27; loss3580.90020942688\n",
      "new best model saved\n",
      "epoch: 28; loss3572.324654817581\n",
      "new best model saved\n",
      "epoch: 29; loss3566.419641494751\n",
      "new best model saved\n",
      "epoch: 30; loss3563.01514005661\n",
      "epoch: 31; loss3565.05757856369\n",
      "epoch: 32; loss3567.240341901779\n",
      "new best model saved\n",
      "epoch: 33; loss3559.2744631767273\n",
      "new best model saved\n",
      "epoch: 34; loss3556.164895057678\n",
      "new best model saved\n",
      "epoch: 35; loss3539.55308675766\n",
      "new best model saved\n",
      "epoch: 36; loss3526.1282761096954\n",
      "new best model saved\n",
      "epoch: 37; loss3521.266146659851\n",
      "epoch: 38; loss3521.8758413791656\n",
      "new best model saved\n",
      "epoch: 39; loss3514.314453601837\n",
      "new best model saved\n",
      "epoch: 40; loss3510.5922627449036\n",
      "epoch: 41; loss3515.2429535388947\n",
      "new best model saved\n",
      "epoch: 42; loss3509.513709783554\n",
      "new best model saved\n",
      "epoch: 43; loss3491.9047741889954\n",
      "new best model saved\n",
      "epoch: 44; loss3487.64488363266\n",
      "new best model saved\n",
      "epoch: 45; loss3486.1668202877045\n",
      "epoch: 46; loss3486.3554537296295\n",
      "new best model saved\n",
      "epoch: 47; loss3482.9723398685455\n",
      "epoch: 48; loss3502.5640137195587\n",
      "epoch: 49; loss3501.773516178131\n",
      "epoch: 50; loss3488.7443265914917\n",
      "epoch: 51; loss3485.2946355342865\n",
      "new best model saved\n",
      "epoch: 52; loss3473.20219540596\n",
      "new best model saved\n",
      "epoch: 53; loss3469.264455318451\n",
      "new best model saved\n",
      "epoch: 54; loss3467.390324115753\n",
      "new best model saved\n",
      "epoch: 55; loss3462.5375146865845\n",
      "epoch: 56; loss3467.273706674576\n",
      "epoch: 57; loss3463.1183915138245\n",
      "epoch: 58; loss3465.0357568264008\n",
      "epoch: 59; loss3486.8503992557526\n",
      "epoch: 60; loss3507.5782136917114\n",
      "epoch: 61; loss3499.136474132538\n",
      "epoch: 62; loss3518.5798394680023\n",
      "epoch: 63; loss3513.4098358154297\n",
      "epoch: 64; loss3505.6700954437256\n",
      "epoch: 65; loss3510.4148349761963\n",
      "epoch: 66; loss3490.499525308609\n",
      "epoch: 67; loss3496.8830840587616\n",
      "epoch: 68; loss3480.0356526374817\n",
      "epoch: 69; loss3469.078149318695\n",
      "new best model saved\n",
      "epoch: 70; loss3458.8161396980286\n",
      "new best model saved\n",
      "epoch: 71; loss3457.56383061409\n",
      "epoch: 72; loss3462.268899202347\n",
      "new best model saved\n",
      "epoch: 73; loss3452.2893846035004\n",
      "new best model saved\n",
      "epoch: 74; loss3447.30876994133\n",
      "epoch: 75; loss3450.961013317108\n",
      "epoch: 76; loss3462.668258666992\n",
      "epoch: 77; loss3475.4778311252594\n",
      "epoch: 78; loss3468.479014158249\n",
      "epoch: 79; loss3456.8386409282684\n",
      "epoch: 80; loss3461.6360816955566\n",
      "epoch: 81; loss3452.3204555511475\n",
      "epoch: 82; loss3448.902711391449\n",
      "epoch: 83; loss3456.943388223648\n",
      "epoch: 84; loss3455.5052642822266\n",
      "epoch: 85; loss3471.5617773532867\n",
      "epoch: 86; loss3460.7873327732086\n",
      "epoch: 87; loss3458.605893135071\n",
      "epoch: 88; loss3466.1875872612\n",
      "epoch: 89; loss3463.760896205902\n",
      "epoch: 90; loss3459.029153585434\n",
      "epoch: 91; loss3489.198080778122\n",
      "epoch: 92; loss3485.369270801544\n",
      "epoch: 93; loss3481.9602065086365\n",
      "epoch: 94; loss3505.053587436676\n",
      "epoch: 95; loss3514.112018108368\n",
      "epoch: 96; loss3504.45894241333\n",
      "epoch: 97; loss3485.1887667179108\n",
      "epoch: 98; loss3462.601634502411\n",
      "epoch: 99; loss3483.7338976860046\n"
     ]
    }
   ],
   "source": [
    "functional.reset_net(model)\n",
    "best_loss = 10000000.0\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for x, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "\n",
    "        if scaler is None:\n",
    "            out_fr = 0.\n",
    "            # for t in range(timesteps):\n",
    "            #     encoded_img = encoder(x)\n",
    "            #     out_fr += model(encoded_img)\n",
    "            # out_fr = out_fr / timesteps\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with amp.autocast():\n",
    "\n",
    "                out_fr = model(x)\n",
    "                # out_fr = out_fr / timesteps\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        # print(out_fr.shape)\n",
    "        # print(label.shape)\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(model) #need to reset the snn before reuse\n",
    "    if train_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f=full_path)\n",
    "        best_loss = train_loss\n",
    "        print('new best model saved')\n",
    "    print('epoch: ' + str(epoch) + '; loss' + str(train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
