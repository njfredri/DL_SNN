{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# spikingjelly.activation_based.examples.conv_fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from spikingjelly.activation_based import encoding\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from torch.cuda import amp\n",
    "import sys\n",
    "import datetime\n",
    "from spikingjelly import visualizing\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetSNN(nn.Module):\n",
    "    def __init__(self, T: int):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.conv_and_fc = nn.Sequential(\n",
    "            layer.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            layer.Flatten(),\n",
    "            layer.Linear(5*5*16, 120),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.Linear(120, 84),\n",
    "            neuron.IFNode(surrogate_function=surrogate.LeakyKReLU()),\n",
    "            layer.Linear(84, 10),\n",
    "            neuron.IFNode(surrogate_function=surrogate.ATan())\n",
    "            )\n",
    "        functional.set_step_mode(self, step_mode='m')\n",
    "    def forward(self, x):\n",
    "        x_seq = x.unsqueeze(0).repeat(self.T, 1, 1, 1, 1)\n",
    "        z = self.conv_and_fc(x_seq)\n",
    "        fr = z.mean(0)\n",
    "        return fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tau = 2.0\n",
    "timesteps = 20\n",
    "model = LeNetSNN(T=timesteps).to(device=device)\n",
    "EPOCHS=100\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/CSNN\"\n",
    "encoder = encoding.PoissonEncoder()\n",
    "batch_size=64\n",
    "num_workers = 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=train_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=True\n",
    "# )\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     dataset=test_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False\n",
    "# )\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CSNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if AMP:\n",
    "    scaler = amp.GradScaler()\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "writer = SummaryWriter(out_dir, purge_step=0)\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write('\\n')\n",
    "    args_txt.write(' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; loss6239.7500829696655\n",
      "epoch: 1; loss6303.500084400177\n",
      "epoch: 2; loss6299.350085258484\n",
      "epoch: 3; loss6298.550093173981\n",
      "epoch: 4; loss6293.550090789795\n",
      "epoch: 5; loss6291.550089836121\n",
      "epoch: 6; loss6295.2500920295715\n",
      "epoch: 7; loss6300.100090503693\n",
      "epoch: 8; loss6292.0500864982605\n",
      "epoch: 9; loss6292.650089263916\n",
      "epoch: 10; loss6295.9000878334045\n",
      "epoch: 11; loss6302.750090122223\n",
      "epoch: 12; loss6304.0000829696655\n",
      "epoch: 13; loss6306.050095081329\n",
      "epoch: 14; loss6303.100093841553\n",
      "epoch: 15; loss6296.200090885162\n",
      "epoch: 16; loss6296.800085544586\n",
      "epoch: 17; loss6306.700092792511\n",
      "epoch: 18; loss6300.600088119507\n",
      "epoch: 19; loss6290.650085449219\n",
      "epoch: 20; loss6297.350081443787\n",
      "epoch: 21; loss6299.700090408325\n",
      "epoch: 22; loss6289.050083637238\n",
      "epoch: 23; loss6299.350088596344\n",
      "epoch: 24; loss6301.700087070465\n",
      "epoch: 25; loss6301.200090408325\n",
      "epoch: 26; loss6301.7500829696655\n",
      "epoch: 27; loss6291.000094413757\n",
      "epoch: 28; loss6296.35008764267\n",
      "epoch: 29; loss6298.6000900268555\n",
      "epoch: 30; loss6300.050090789795\n",
      "epoch: 31; loss6302.700086593628\n",
      "epoch: 32; loss6294.500088214874\n",
      "epoch: 33; loss6293.75009059906\n",
      "epoch: 34; loss6296.700096130371\n",
      "epoch: 35; loss6303.700102329254\n",
      "epoch: 36; loss6294.150084972382\n",
      "epoch: 37; loss6290.5000872612\n",
      "epoch: 38; loss6286.100095272064\n",
      "epoch: 39; loss6296.350092887878\n",
      "epoch: 40; loss6292.350088119507\n",
      "epoch: 41; loss6304.100089073181\n",
      "epoch: 42; loss6296.050086021423\n",
      "epoch: 43; loss6298.100088596344\n",
      "epoch: 44; loss6295.200088024139\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "functional.reset_net(model)\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for x, label in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "\n",
    "        if scaler is None:\n",
    "            out_fr = 0.\n",
    "            # for t in range(timesteps):\n",
    "            #     encoded_img = encoder(x)\n",
    "            #     out_fr += model(encoded_img)\n",
    "            # out_fr = out_fr / timesteps\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with amp.autocast():\n",
    "\n",
    "                out_fr = model(x)\n",
    "                # out_fr = out_fr / timesteps\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        # print(out_fr.shape)\n",
    "        # print(label.shape)\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(model) #need to reset the snn before reuse\n",
    "    print('epoch: ' + str(epoch) + '; loss' + str(train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
