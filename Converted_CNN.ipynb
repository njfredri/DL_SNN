{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# spikingjelly.activation_based.examples.conv_fashion_mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from spikingjelly.activation_based import neuron, functional, surrogate, layer, monitor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from torch.cuda import amp\n",
    "import sys\n",
    "import datetime\n",
    "from spikingjelly import visualizing\n",
    "from spikingjelly.activation_based import ann2snn\n",
    "from spikingjelly.activation_based import encoding\n",
    "\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model needs to account for 2 things:\n",
    "\n",
    "1. ANN proposes Batch Normalization for fast training and convergence. Batch normalization aims to normalize the ANN output to 0 mean, which is contrary to the properties of SNNs. Therefore, the parameters of BN can be absorbed into the previous parameter layers (Linear, Conv2d)\n",
    "\n",
    "2. According to the transformation theory, the input and output of each layer of ANN need to be limited to the range of [0,1], which requires scaling the parameters (model normalization)\n",
    "\n",
    "3. There is not a good way to use MaxPooling. AvgPool is recommended instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet_Modified(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet_Modified, self).__init__()\n",
    "        #5x5 kernal on 28x28 image. Should have 2 padding for \"32x32\" image\n",
    "        self.c1 = nn.Conv2d(in_channels=1, kernel_size=5, padding=2, out_channels=6) #results in 28x28 in 6 channels. Should have 1 channel-in bc it is one image at first\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=6, eps=1e-3)\n",
    "        self.ap1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        #Relu 28x28 -> 28x28\n",
    "        #avg pool 28x28 -> 14x14 (stride=2)\n",
    "        self.c2 = nn.Conv2d(in_channels=6, kernel_size=5, out_channels=16) #6 channels to 16 channels. 14x14 -> 10x10 with 5x5kernel\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=16, eps=1e-3)\n",
    "        self.ap2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        #avg pool 10x10 -> 5x5\n",
    "        self.fc1 = nn.Linear(25*16, 120) #5x5 images, 16 channels in. 120 out\n",
    "        self.fc2 = nn.Linear(120, 84) #120 -> 84\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self.c1,\n",
    "            self.bn1,\n",
    "            nn.ReLU(),\n",
    "            self.ap1,\n",
    "            self.c2,\n",
    "            self.bn2,\n",
    "            nn.ReLU(),\n",
    "            self.ap2,\n",
    "            nn.Flatten(),\n",
    "            self.fc1,\n",
    "            nn.ReLU(),\n",
    "            self.fc2,\n",
    "            nn.ReLU(),\n",
    "            self.fc3,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.network(x)\n",
    "        return x\n",
    "\n",
    "    def name(self):\n",
    "        return \"LeNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "model_ann = LeNet_Modified().to(device=device)\n",
    "optimizer = torch.optim.Adam(model_ann.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the FMNIST dataset and then create the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './FMNIST'\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root=root,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> epoch: 0, train loss: 0.014192\n",
      "==>>> epoch: 1, train loss: 0.009726\n",
      "==>>> epoch: 2, train loss: 0.008742\n",
      "==>>> epoch: 3, train loss: 0.007902\n",
      "==>>> epoch: 4, train loss: 0.007462\n",
      "==>>> epoch: 5, train loss: 0.006932\n",
      "==>>> epoch: 6, train loss: 0.006502\n",
      "==>>> epoch: 7, train loss: 0.006204\n",
      "==>>> epoch: 8, train loss: 0.005820\n",
      "==>>> epoch: 9, train loss: 0.005489\n",
      "==>>> epoch: 10, train loss: 0.005226\n",
      "==>>> epoch: 11, train loss: 0.004959\n",
      "==>>> epoch: 12, train loss: 0.004651\n",
      "==>>> epoch: 13, train loss: 0.004485\n",
      "==>>> epoch: 14, train loss: 0.004230\n",
      "==>>> epoch: 15, train loss: 0.004021\n",
      "==>>> epoch: 16, train loss: 0.003779\n",
      "==>>> epoch: 17, train loss: 0.003572\n",
      "==>>> epoch: 18, train loss: 0.003467\n",
      "==>>> epoch: 19, train loss: 0.003244\n",
      "==>>> epoch: 20, train loss: 0.003070\n",
      "==>>> epoch: 21, train loss: 0.002970\n",
      "==>>> epoch: 22, train loss: 0.002856\n",
      "==>>> epoch: 23, train loss: 0.002715\n",
      "==>>> epoch: 24, train loss: 0.002540\n",
      "==>>> epoch: 25, train loss: 0.002549\n",
      "==>>> epoch: 26, train loss: 0.002308\n",
      "==>>> epoch: 27, train loss: 0.002262\n",
      "==>>> epoch: 28, train loss: 0.002120\n",
      "==>>> epoch: 29, train loss: 0.002108\n",
      "==>>> epoch: 30, train loss: 0.002003\n",
      "==>>> epoch: 31, train loss: 0.001930\n",
      "==>>> epoch: 32, train loss: 0.001897\n",
      "==>>> epoch: 33, train loss: 0.001843\n",
      "==>>> epoch: 34, train loss: 0.001767\n",
      "==>>> epoch: 35, train loss: 0.001628\n",
      "==>>> epoch: 36, train loss: 0.001651\n",
      "==>>> epoch: 37, train loss: 0.001550\n",
      "==>>> epoch: 38, train loss: 0.001534\n",
      "==>>> epoch: 39, train loss: 0.001573\n",
      "==>>> epoch: 40, train loss: 0.001366\n",
      "==>>> epoch: 41, train loss: 0.001386\n",
      "==>>> epoch: 42, train loss: 0.001360\n",
      "==>>> epoch: 43, train loss: 0.001305\n",
      "==>>> epoch: 44, train loss: 0.001337\n",
      "==>>> epoch: 45, train loss: 0.001162\n",
      "==>>> epoch: 46, train loss: 0.001292\n",
      "==>>> epoch: 47, train loss: 0.001159\n",
      "==>>> epoch: 48, train loss: 0.001134\n",
      "==>>> epoch: 49, train loss: 0.001163\n",
      "==>>> epoch: 50, train loss: 0.001008\n",
      "==>>> epoch: 51, train loss: 0.001106\n",
      "==>>> epoch: 52, train loss: 0.001113\n",
      "==>>> epoch: 53, train loss: 0.001079\n",
      "==>>> epoch: 54, train loss: 0.000946\n",
      "==>>> epoch: 55, train loss: 0.001043\n",
      "==>>> epoch: 56, train loss: 0.000954\n",
      "==>>> epoch: 57, train loss: 0.000923\n",
      "==>>> epoch: 58, train loss: 0.000886\n",
      "==>>> epoch: 59, train loss: 0.000995\n",
      "==>>> epoch: 60, train loss: 0.000902\n",
      "==>>> epoch: 61, train loss: 0.000921\n",
      "==>>> epoch: 62, train loss: 0.000820\n",
      "==>>> epoch: 63, train loss: 0.000893\n",
      "==>>> epoch: 64, train loss: 0.000864\n",
      "==>>> epoch: 65, train loss: 0.000837\n",
      "==>>> epoch: 66, train loss: 0.000780\n",
      "==>>> epoch: 67, train loss: 0.000779\n",
      "==>>> epoch: 68, train loss: 0.000814\n",
      "==>>> epoch: 69, train loss: 0.000743\n",
      "==>>> epoch: 70, train loss: 0.000825\n",
      "==>>> epoch: 71, train loss: 0.000784\n",
      "==>>> epoch: 72, train loss: 0.000702\n",
      "==>>> epoch: 73, train loss: 0.000678\n",
      "==>>> epoch: 74, train loss: 0.000774\n",
      "==>>> epoch: 75, train loss: 0.000666\n",
      "==>>> epoch: 76, train loss: 0.000730\n",
      "==>>> epoch: 77, train loss: 0.000738\n",
      "==>>> epoch: 78, train loss: 0.000660\n",
      "==>>> epoch: 79, train loss: 0.000637\n",
      "==>>> epoch: 80, train loss: 0.000659\n",
      "==>>> epoch: 81, train loss: 0.000654\n",
      "==>>> epoch: 82, train loss: 0.000748\n",
      "==>>> epoch: 83, train loss: 0.000584\n",
      "==>>> epoch: 84, train loss: 0.000674\n",
      "==>>> epoch: 85, train loss: 0.000602\n",
      "==>>> epoch: 86, train loss: 0.000622\n",
      "==>>> epoch: 87, train loss: 0.000651\n",
      "==>>> epoch: 88, train loss: 0.000573\n",
      "==>>> epoch: 89, train loss: 0.000684\n",
      "==>>> epoch: 90, train loss: 0.000563\n",
      "==>>> epoch: 91, train loss: 0.000635\n",
      "==>>> epoch: 92, train loss: 0.000605\n",
      "==>>> epoch: 93, train loss: 0.000555\n",
      "==>>> epoch: 94, train loss: 0.000651\n",
      "==>>> epoch: 95, train loss: 0.000513\n",
      "==>>> epoch: 96, train loss: 0.000623\n",
      "==>>> epoch: 97, train loss: 0.000593\n",
      "==>>> epoch: 98, train loss: 0.000561\n",
      "==>>> epoch: 99, train loss: 0.000551\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    # trainning\n",
    "    total_loss = 0\n",
    "    for batch_idx, (x, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, target = x.cuda(), target.cuda()\n",
    "        out = model_ann(x)\n",
    "        loss = criterion(out, target)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / len(train_set)\n",
    "    print(f'==>>> epoch: {epoch}, train loss: {avg_loss:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = './Models'\n",
    "if not os.path.exists(file_dir):\n",
    "    os.makedirs(file_dir)\n",
    "    print(f'Mkdir {file_dir}.')\n",
    "full_path = file_dir + '/LeNET_ann.pt'\n",
    "torch.save(model_ann.state_dict(), f=full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_ann.eval()\n",
    "total_loss = 0\n",
    "correct_cnt = 0\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    out = model_ann(x)\n",
    "    loss = criterion(out, target)\n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_cnt += (pred_label == target).sum()\n",
    "    # smooth average\n",
    "    total_loss += loss.item()\n",
    "avg_loss = total_loss / len(test_set)\n",
    "avg_acc = correct_cnt / len(test_set)\n",
    "print(f'test loss: {avg_loss:.6f}, test accuracy: {avg_acc:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Model from ANN to SNN\n",
    "3 different modes:\n",
    "* MaxNorm\n",
    "* 99.9%\n",
    "* scaling mode (float 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1875 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1875/1875 [00:08<00:00, 208.98it/s]\n"
     ]
    }
   ],
   "source": [
    "model_converter = ann2snn.Converter(mode='max', dataloader=train_loader)\n",
    "model_snn = model_converter(model_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet_Modified(\n",
      "  (c1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (network): Module(\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (ap1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (c2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (ap2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "  (snn tailor): Module(\n",
      "    (0): Module(\n",
      "      (0): VoltageScaler(0.129585)\n",
      "      (1): IFNode(\n",
      "        v_threshold=1.0, v_reset=None, detach_reset=False, step_mode=s, backend=torch\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "      (2): VoltageScaler(7.716962)\n",
      "    )\n",
      "    (1): Module(\n",
      "      (0): VoltageScaler(0.203869)\n",
      "      (1): IFNode(\n",
      "        v_threshold=1.0, v_reset=None, detach_reset=False, step_mode=s, backend=torch\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "      (2): VoltageScaler(4.905120)\n",
      "    )\n",
      "    (2): Module(\n",
      "      (0): VoltageScaler(0.080672)\n",
      "      (1): IFNode(\n",
      "        v_threshold=1.0, v_reset=None, detach_reset=False, step_mode=s, backend=torch\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "      (2): VoltageScaler(12.395879)\n",
      "    )\n",
      "    (3): Module(\n",
      "      (0): VoltageScaler(0.034253)\n",
      "      (1): IFNode(\n",
      "        v_threshold=1.0, v_reset=None, detach_reset=False, step_mode=s, backend=torch\n",
      "        (surrogate_function): Sigmoid(alpha=4.0, spiking=True)\n",
      "      )\n",
      "      (2): VoltageScaler(29.194105)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x):\n",
      "    c1 = self.c1(x);  x = None\n",
      "    snn_tailor_0_1 = getattr(getattr(getattr(self, \"snn tailor\"), \"0\"), \"0\")(c1);  c1 = None\n",
      "    snn_tailor_0_2 = getattr(getattr(getattr(self, \"snn tailor\"), \"0\"), \"1\")(snn_tailor_0_1);  snn_tailor_0_1 = None\n",
      "    snn_tailor_0_3 = getattr(getattr(getattr(self, \"snn tailor\"), \"0\"), \"2\")(snn_tailor_0_2);  snn_tailor_0_2 = None\n",
      "    ap1 = self.ap1(snn_tailor_0_3);  snn_tailor_0_3 = None\n",
      "    c2 = self.c2(ap1);  ap1 = None\n",
      "    snn_tailor_1_1 = getattr(getattr(getattr(self, \"snn tailor\"), \"1\"), \"0\")(c2);  c2 = None\n",
      "    snn_tailor_1_2 = getattr(getattr(getattr(self, \"snn tailor\"), \"1\"), \"1\")(snn_tailor_1_1);  snn_tailor_1_1 = None\n",
      "    snn_tailor_1_3 = getattr(getattr(getattr(self, \"snn tailor\"), \"1\"), \"2\")(snn_tailor_1_2);  snn_tailor_1_2 = None\n",
      "    ap2 = self.ap2(snn_tailor_1_3);  snn_tailor_1_3 = None\n",
      "    network_8 = getattr(self.network, \"8\")(ap2);  ap2 = None\n",
      "    fc1 = self.fc1(network_8);  network_8 = None\n",
      "    snn_tailor_2_1 = getattr(getattr(getattr(self, \"snn tailor\"), \"2\"), \"0\")(fc1);  fc1 = None\n",
      "    snn_tailor_2_2 = getattr(getattr(getattr(self, \"snn tailor\"), \"2\"), \"1\")(snn_tailor_2_1);  snn_tailor_2_1 = None\n",
      "    snn_tailor_2_3 = getattr(getattr(getattr(self, \"snn tailor\"), \"2\"), \"2\")(snn_tailor_2_2);  snn_tailor_2_2 = None\n",
      "    fc2 = self.fc2(snn_tailor_2_3);  snn_tailor_2_3 = None\n",
      "    snn_tailor_3_1 = getattr(getattr(getattr(self, \"snn tailor\"), \"3\"), \"0\")(fc2);  fc2 = None\n",
      "    snn_tailor_3_2 = getattr(getattr(getattr(self, \"snn tailor\"), \"3\"), \"1\")(snn_tailor_3_1);  snn_tailor_3_1 = None\n",
      "    snn_tailor_3_3 = getattr(getattr(getattr(self, \"snn tailor\"), \"3\"), \"2\")(snn_tailor_3_2);  snn_tailor_3_2 = None\n",
      "    fc3 = self.fc3(snn_tailor_3_3);  snn_tailor_3_3 = None\n",
      "    return fc3\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(model_snn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run test on SNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.905000\n"
     ]
    }
   ],
   "source": [
    "model_ann.eval()\n",
    "T = 100\n",
    "total_loss = 0\n",
    "correct_cnt = 0\n",
    "for batch_idx, (x, target) in enumerate(test_loader):\n",
    "    x, target = x.to(device), target.to(device)\n",
    "    #reset before new eval\n",
    "    for m in model_snn.modules():\n",
    "        if hasattr(m, 'reset'):\n",
    "            m.reset()\n",
    "    #evaluate for a number of timesteps\n",
    "    for t in range (T):\n",
    "        if t ==0:\n",
    "            out = model_snn(x)\n",
    "        else:\n",
    "            out += model_snn(x)\n",
    "    # loss = criterion(out, target)\n",
    "    _, pred_label = torch.max(out, 1)\n",
    "    correct_cnt += (pred_label == target).sum()\n",
    "    # smooth average\n",
    "    total_loss += loss.item()\n",
    "# avg_loss = total_loss / len(test_set)\n",
    "avg_acc = correct_cnt / len(test_set)\n",
    "print(f'test accuracy: {avg_acc:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for spike count.\n",
    "Go through test set again, but with 1 img at a time.\n",
    "For everytime step, record membrane potential and number of spikes (if seeing spikes is possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_monitor = monitor.OutputMonitor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
