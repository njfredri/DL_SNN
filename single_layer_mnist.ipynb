{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make simple snn to classify MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import sys\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.cuda import amp\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from spikingjelly.activation_based import neuron, layer, functional, monitor\n",
    "from spikingjelly.activation_based import surrogate, encoding\n",
    "\n",
    "device = None\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            layer.Linear(28*28, 10, bias=False),\n",
    "            neuron.LIFNode(tau=tau, surrogate_function=surrogate.ATan())\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.layer1(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model with a default tau value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 2.0 #membrane time constant. \n",
    "#^ A default value in some of the spikingjelly repo files.\n",
    "model = SNN(tau=tau).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the MNIST dataset and make train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data'\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "num_workers=4 #todo: see if this can be increased with more cores\n",
    "train_data_loader = data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_data_loader = data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "AMP=True #automatic mixed precision training\n",
    "lr= 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "out_dir = \"./outputs/SLM\"\n",
    "encoder = encoding.PoissonEncoder()\n",
    "timesteps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make settings for AMP and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = None\n",
    "if AMP:\n",
    "    scaler = amp.GradScaler()\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    print(f'Mkdir {out_dir}.')\n",
    "\n",
    "writer = SummaryWriter(out_dir, purge_step=0)\n",
    "with open(os.path.join(out_dir, 'args.txt'), 'w', encoding='utf-8') as args_txt:\n",
    "    args_txt.write('\\n')\n",
    "    args_txt.write(' '.join(sys.argv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0; loss1473.2269598692656\n",
      "epoch: 1; loss1075.422732412815\n",
      "epoch: 2; loss1011.1290106400847\n",
      "epoch: 3; loss976.7880894318223\n",
      "epoch: 4; loss954.6542589962482\n",
      "epoch: 5; loss939.7806394323707\n",
      "epoch: 6; loss925.6065278202295\n",
      "epoch: 7; loss915.5676494017243\n",
      "epoch: 8; loss906.2968903928995\n",
      "epoch: 9; loss900.0954683721066\n",
      "epoch: 10; loss892.9173497408628\n",
      "epoch: 11; loss887.5273577496409\n",
      "epoch: 12; loss881.0368397347629\n",
      "epoch: 13; loss877.2217765562236\n",
      "epoch: 14; loss872.8955892696977\n",
      "epoch: 15; loss869.4887975566089\n",
      "epoch: 16; loss866.014687821269\n",
      "epoch: 17; loss863.2271680906415\n",
      "epoch: 18; loss861.9286185540259\n",
      "epoch: 19; loss858.624628201127\n",
      "epoch: 20; loss854.8929985538125\n",
      "epoch: 21; loss854.0605385005474\n",
      "epoch: 22; loss851.3097783252597\n",
      "epoch: 23; loss850.2689578793943\n",
      "epoch: 24; loss846.8791446890682\n",
      "epoch: 25; loss844.2525178343058\n",
      "epoch: 26; loss843.1649576984346\n",
      "epoch: 27; loss843.2444987073541\n",
      "epoch: 28; loss840.7092669159174\n",
      "epoch: 29; loss839.7281689345837\n",
      "epoch: 30; loss838.1195760630071\n",
      "epoch: 31; loss837.2025465890765\n",
      "epoch: 32; loss837.7559580355883\n",
      "epoch: 33; loss835.5021688453853\n",
      "epoch: 34; loss834.7020465955138\n",
      "epoch: 35; loss831.6933483891189\n",
      "epoch: 36; loss832.6931478008628\n",
      "epoch: 37; loss829.0737278573215\n",
      "epoch: 38; loss830.128467798233\n",
      "epoch: 39; loss827.7248262166977\n",
      "epoch: 40; loss825.6517765596509\n",
      "epoch: 41; loss825.8996869344264\n",
      "epoch: 42; loss824.8261485528201\n",
      "epoch: 43; loss826.0337371118367\n",
      "epoch: 44; loss824.8892982304096\n",
      "epoch: 45; loss823.7418784853071\n",
      "epoch: 46; loss822.5442181378603\n",
      "epoch: 47; loss821.6292662918568\n",
      "epoch: 48; loss821.0467976592481\n",
      "epoch: 49; loss821.697456844151\n",
      "epoch: 50; loss819.5288180448115\n",
      "epoch: 51; loss819.3884270545095\n",
      "epoch: 52; loss819.008417148143\n",
      "epoch: 53; loss818.3070161379874\n",
      "epoch: 54; loss817.5898763220757\n",
      "epoch: 55; loss816.5446671061218\n",
      "epoch: 56; loss816.4163686931133\n",
      "epoch: 57; loss816.6652765180916\n",
      "epoch: 58; loss816.1501662991941\n",
      "epoch: 59; loss813.4121285118163\n",
      "epoch: 60; loss813.5195174627006\n",
      "epoch: 61; loss814.0201869681478\n",
      "epoch: 62; loss812.2287575528026\n",
      "epoch: 63; loss811.7368667162955\n",
      "epoch: 64; loss812.7526765726507\n",
      "epoch: 65; loss811.4009681493044\n",
      "epoch: 66; loss813.3128782529384\n",
      "epoch: 67; loss809.8964362591505\n",
      "epoch: 68; loss809.7953170910478\n",
      "epoch: 69; loss810.097267659381\n",
      "epoch: 70; loss808.5466778837144\n",
      "epoch: 71; loss810.0282567515969\n",
      "epoch: 72; loss809.1175775974989\n",
      "epoch: 73; loss809.3799790591002\n",
      "epoch: 74; loss806.6589656919241\n",
      "epoch: 75; loss806.5317968223244\n",
      "epoch: 76; loss806.766266901046\n",
      "epoch: 77; loss807.0781763456762\n",
      "epoch: 78; loss805.9689053744078\n",
      "epoch: 79; loss805.6788484156132\n",
      "epoch: 80; loss805.6092879567295\n",
      "epoch: 81; loss804.2722980044782\n",
      "epoch: 82; loss805.6334071755409\n",
      "epoch: 83; loss803.5642881467938\n",
      "epoch: 84; loss804.6054482795298\n",
      "epoch: 85; loss804.1350063476712\n",
      "epoch: 86; loss803.0503163188696\n",
      "epoch: 87; loss804.9193067885935\n",
      "epoch: 88; loss803.0043078139424\n",
      "epoch: 89; loss803.9921069256961\n",
      "epoch: 90; loss803.8067064285278\n",
      "epoch: 91; loss802.2530753072351\n",
      "epoch: 92; loss802.383756481111\n",
      "epoch: 93; loss802.8680176762864\n",
      "epoch: 94; loss802.443128619343\n",
      "epoch: 95; loss801.1927968300879\n",
      "epoch: 96; loss799.2213070616126\n",
      "epoch: 97; loss800.2213478684425\n",
      "epoch: 98; loss801.1910478435457\n",
      "epoch: 99; loss801.216728232801\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_samples = 0\n",
    "    for x, label in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "\n",
    "        if scaler is None:\n",
    "            out_fr = 0.\n",
    "            for t in range(timesteps):\n",
    "                encoded_img = encoder(x)\n",
    "                out_fr += model(encoded_img)\n",
    "            out_fr = out_fr / timesteps\n",
    "            loss = F.mse_loss(out_fr, label_onehot)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            with amp.autocast():\n",
    "                out_fr = 0.\n",
    "                for t in range(timesteps):\n",
    "                    encoded_img = encoder(x)\n",
    "                    out_fr += model(encoded_img)\n",
    "                out_fr = out_fr / timesteps\n",
    "                loss = F.mse_loss(out_fr, label_onehot)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        \n",
    "        train_samples += label.numel()\n",
    "        train_loss += loss.item() * label.numel()\n",
    "        train_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "\n",
    "        functional.reset_net(model) #need to reset the snn before reuse\n",
    "    print('epoch: ' + str(epoch) + '; loss' + str(train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9283\n"
     ]
    }
   ],
   "source": [
    "#test the accuracy\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "with torch.no_grad():\n",
    "    for x, label in test_data_loader:\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(timesteps):\n",
    "            encoded_img = encoder(x)\n",
    "            out_fr += model(encoded_img)\n",
    "        out_fr = out_fr / timesteps\n",
    "        loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "        test_samples += label.numel()\n",
    "        test_loss += loss.item() * label.numel()\n",
    "        test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "        functional.reset_net(model)\n",
    "test_time = time.time()\n",
    "# test_speed = test_samples / (test_time - train_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "writer.add_scalar('test_loss', test_loss, epoch)\n",
    "writer.add_scalar('test_acc', test_acc, epoch)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Spike Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test the accuracy\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "test_acc = 0\n",
    "test_samples = 0\n",
    "with torch.no_grad():\n",
    "    for x, label in test_data_loader:\n",
    "        x = x.to(device)\n",
    "        label = label.to(device)\n",
    "        label_onehot = F.one_hot(label, 10).float()\n",
    "        out_fr = 0.\n",
    "        for t in range(timesteps):\n",
    "            encoded_img = encoder(x)\n",
    "            out_fr += model(encoded_img)\n",
    "        out_fr = out_fr / timesteps\n",
    "        loss = F.mse_loss(out_fr, label_onehot)\n",
    "\n",
    "        test_samples += label.numel()\n",
    "        test_loss += loss.item() * label.numel()\n",
    "        test_acc += (out_fr.argmax(1) == label).float().sum().item()\n",
    "        functional.reset_net(model)\n",
    "test_time = time.time()\n",
    "# test_speed = test_samples / (test_time - train_time)\n",
    "test_loss /= test_samples\n",
    "test_acc /= test_samples\n",
    "writer.add_scalar('test_loss', test_loss, epoch)\n",
    "writer.add_scalar('test_acc', test_acc, epoch)\n",
    "print(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
